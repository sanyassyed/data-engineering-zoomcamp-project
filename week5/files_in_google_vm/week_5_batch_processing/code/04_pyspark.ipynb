{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c67f1a55",
   "metadata": {},
   "source": [
    "In this file \n",
    "* Part-1\n",
    "    * Create a SparkSession object\n",
    "    * Download a zipped csv file from the web into the `data` folder\n",
    "    * Convert zipped csv to csv (not required, both pandas and PySpark can read zipped csv files automatically)\n",
    "    * Read the csv as a PySpark DataFrame object\n",
    "    * View the top 5 rows of the PySpark DataFrame\n",
    "    * Because PySpark sets all columns to type string we try and extract the correct schema for the dataset by doing the following:\n",
    "        1. Reading a few rows of the csv file to a PANDAS DataFrame object\n",
    "        2. Reading the Pandas DataFrame into a PySpark DataFrame object and reading the schema (because Spark by default infers the schema based on the   pandas data types TO PySpark data types) \n",
    "        3. Then using the extractd schema, we edit it in VS Code\n",
    "        4. Pass this extracted and edited schema as a variable while reading the csv file as a PySpark DataFrame object\n",
    "    * Set the partition size of the PySpark Df as 24\n",
    "    * Write the PySpark DF as a parquet file to a folder as 24 partitions\n",
    "* Part-2\n",
    "    * Read the parquet partitions into a PySpark DF\n",
    "    * Experiment with some PySpark DF functions\n",
    "        * Lazy Functions\n",
    "        * Active Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc0b20d",
   "metadata": {},
   "source": [
    "# Part-1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a88bdf",
   "metadata": {},
   "source": [
    "## Create a SparkSession object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bceb4dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ac160a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for setting the directories\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91c32039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/sanyashireen/spark/spark-3.2.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/02/23 18:12:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Creating a SparkSession object\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName('test04').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e2309b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9990f55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: /home/sanyashireen/week_5_batch_processing/code\n",
      "Data Directory: /home/sanyashireen/week_5_batch_processing/data\n"
     ]
    }
   ],
   "source": [
    "# Setting the working directory\n",
    "working_dir = os.getcwd()\n",
    "parent_working_dir = os.path.dirname(working_dir) \n",
    "data_dir = os.path.join(parent_working_dir, 'data')\n",
    "print(f'Current Directory: {working_dir}')\n",
    "print(f'Data Directory: {data_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "483faaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: /home/sanyashireen/week_5_batch_processing/data\n"
     ]
    }
   ],
   "source": [
    "# Moving to the data directory\n",
    "os.chdir(data_dir)\n",
    "print(f'Current Directory: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4793baa6",
   "metadata": {},
   "source": [
    "## Download a zipped csv file from the web into the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d124b209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fhvhv  head.csv  taxi+_zone_lookup.csv\tzones\n",
      "--2023-02-23 18:14:31--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-01.csv.gz\n",
      "Resolving github.com (github.com)... 140.82.114.4\n",
      "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/035746e8-4e24-47e8-a3ce-edcf6d1b11c7?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230223%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230223T181431Z&X-Amz-Expires=300&X-Amz-Signature=0d4d7c0ed042c37917e5e72aa5d2a90d88bd0716a1b41f80165d6bff05ce9822&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhvhv_tripdata_2021-01.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2023-02-23 18:14:31--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/035746e8-4e24-47e8-a3ce-edcf6d1b11c7?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230223%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230223T181431Z&X-Amz-Expires=300&X-Amz-Signature=0d4d7c0ed042c37917e5e72aa5d2a90d88bd0716a1b41f80165d6bff05ce9822&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhvhv_tripdata_2021-01.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 129967421 (124M) [application/octet-stream]\n",
      "Saving to: ‘fhvhv_tripdata_2021-01.csv.gz’\n",
      "\n",
      "fhvhv_tripdata_2021 100%[===================>] 123.95M  75.1MB/s    in 1.7s    \n",
      "\n",
      "2023-02-23 18:14:33 (75.1 MB/s) - ‘fhvhv_tripdata_2021-01.csv.gz’ saved [129967421/129967421]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking we are in the data directory and \n",
    "# extracting zipped csv file from the web\n",
    "!ls\n",
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-01.csv.gz \n",
    "\n",
    "# Use the `-P` option flag to specify the output folder if different from current working directory \n",
    "#-P '/home/sanyashireen/week_5_batch_processing/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e33e284b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fhvhv  fhvhv_tripdata_2021-01.csv.gz  head.csv\ttaxi+_zone_lookup.csv  zones\r\n"
     ]
    }
   ],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf9d6d4",
   "metadata": {},
   "source": [
    "## Convert zipped csv to csv (not required, both pandas and PySpark can read zipped csv files automatically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "409644d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fhvhv_tripdata_2021-01.csv.gz:\t 82.7% -- replaced with fhvhv_tripdata_2021-01.csv\n"
     ]
    }
   ],
   "source": [
    "# Saving the zipped file as a csv - this is not required as both pandas and PySpark can automatically read zipped csv's\n",
    "# I did it because I was not able to execute the wc command below\n",
    "#!gunzip -v fhvhv_tripdata_2021-01.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84224834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11908469 fhvhv_tripdata_2021-01.csv\r\n"
     ]
    }
   ],
   "source": [
    "#!wc -l fhvhv_tripdata_2021-01.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9809d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11908469\n",
      "hvfhs_license_num,dispatching_base_num,pickup_datetime,dropoff_datetime,PULocationID,DOLocationID,SR_Flag\n",
      "HV0003,B02682,2021-01-01 00:33:44,2021-01-01 00:49:07,230,166,\n",
      "HV0003,B02682,2021-01-01 00:55:19,2021-01-01 01:18:21,152,167,\n",
      "HV0003,B02764,2021-01-01 00:23:56,2021-01-01 00:38:05,233,142,\n",
      "HV0003,B02764,2021-01-01 00:42:51,2021-01-01 00:45:50,142,143,\n",
      "HV0003,B02764,2021-01-01 00:48:14,2021-01-01 01:08:42,143,78,\n",
      "HV0005,B02510,2021-01-01 00:06:59,2021-01-01 00:43:01,88,42,\n",
      "HV0005,B02510,2021-01-01 00:50:00,2021-01-01 01:04:57,42,151,\n",
      "HV0003,B02764,2021-01-01 00:14:30,2021-01-01 00:50:27,71,226,\n",
      "HV0003,B02875,2021-01-01 00:22:54,2021-01-01 00:30:20,112,255,\n",
      "\n",
      "gzip: stdout: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "# Combine the above two commands into one as to view the number of records and first 10 rows\n",
    "!zcat fhvhv_tripdata_2021-01.csv.gz | wc -l\n",
    "!zcat fhvhv_tripdata_2021-01.csv.gz | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f06906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the file name to a variable\n",
    "data_file = 'fhvhv_tripdata_2021-01.csv.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4125bb0a",
   "metadata": {},
   "source": [
    "## Read the csv as a PySpark DataFrame object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89ae721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the zipped CSV as a PySpark DF\n",
    "df = spark.read.option(\"header\", \"true\").csv(f'{data_dir}/{data_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e2ba59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5f99f9",
   "metadata": {},
   "source": [
    "## View the top 5 rows of the PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9332367b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02682|2021-01-01 00:33:44|2021-01-01 00:49:07|         230|         166|   null|\n",
      "|           HV0003|              B02682|2021-01-01 00:55:19|2021-01-01 01:18:21|         152|         167|   null|\n",
      "|           HV0003|              B02764|2021-01-01 00:23:56|2021-01-01 00:38:05|         233|         142|   null|\n",
      "|           HV0003|              B02764|2021-01-01 00:42:51|2021-01-01 00:45:50|         142|         143|   null|\n",
      "|           HV0003|              B02764|2021-01-01 00:48:14|2021-01-01 01:08:42|         143|          78|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Viewing the first 5 rows of the PySpark DF as a table\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fd11a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime='2021-01-01 00:33:44', dropoff_datetime='2021-01-01 00:49:07', PULocationID='230', DOLocationID='166', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime='2021-01-01 00:55:19', dropoff_datetime='2021-01-01 01:18:21', PULocationID='152', DOLocationID='167', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime='2021-01-01 00:23:56', dropoff_datetime='2021-01-01 00:38:05', PULocationID='233', DOLocationID='142', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime='2021-01-01 00:42:51', dropoff_datetime='2021-01-01 00:45:50', PULocationID='142', DOLocationID='143', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime='2021-01-01 00:48:14', dropoff_datetime='2021-01-01 01:08:42', PULocationID='143', DOLocationID='78', SR_Flag=None)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the first few rows as a list\n",
    "# Note: Spark saves all the data as strings, does that for numeric columns too\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdc3f76",
   "metadata": {},
   "source": [
    "### Looking at the datatypes of the various elements returned by the functions PYSpark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50166a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "046e5762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.head(5)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd4c9b5",
   "metadata": {},
   "source": [
    "### Experimenting some function of the PySpark DF object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf6a5002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.describe of DataFrame[hvfhs_license_num: string, dispatching_base_num: string, pickup_datetime: string, dropoff_datetime: string, PULocationID: string, DOLocationID: string, SR_Flag: string]>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d4206d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "method"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49e58954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is not a pandas dataframe rather a pyspark sql dataframe\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b98d977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(hvfhs_license_num,StringType,true),StructField(dispatching_base_num,StringType,true),StructField(pickup_datetime,StringType,true),StructField(dropoff_datetime,StringType,true),StructField(PULocationID,StringType,true),StructField(DOLocationID,StringType,true),StructField(SR_Flag,StringType,true)))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As we can everything is a string\n",
    "df.schema"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7cf86141",
   "metadata": {},
   "source": [
    "## Creating Pandas DF with fewer rows to extract ideal schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6003e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First creating a smaller file and save it into head.csv\n",
    "#!export data_folder=/home/sanyashireen/week_5_batch_processing/data\n",
    "#!export data_file=fhvhv_tripdata_2021-01.csv\n",
    "#!head -n 100 \"${data_folder}/${data_file}\" > \"${data_folder}/head.csv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e9679a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "gzip: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "# Unzip the zipped csv and copy first 101 rows into a new file head.csv\n",
    "!zcat fhvhv_tripdata_2021-01.csv.gz | head -n 101 > head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b118925c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hvfhs_license_num,dispatching_base_num,pickup_datetime,dropoff_datetime,PULocationID,DOLocationID,SR_Flag\r\n",
      "\r\n",
      "HV0003,B02682,2021-01-01 00:33:44,2021-01-01 00:49:07,230,166,\r\n",
      "\r\n",
      "HV0003,B02682,2021-01-01 00:55:19,2021-01-01 01:18:21,152,167,\r\n",
      "\r\n",
      "HV0003,B02764,2021-01-01 00:23:56,2021-01-01 00:38:05,233,142,\r\n",
      "\r\n",
      "HV0003,B02764,2021-01-01 00:42:51,2021-01-01 00:45:50,142,143,\r\n",
      "\r\n",
      "HV0003,B02764,2021-01-01 00:48:14,2021-01-01 01:08:42,143,78,\r\n",
      "\r\n",
      "HV0005,B02510,2021-01-01 00:06:59,2021-01-01 00:43:01,88,42,\r\n",
      "\r\n",
      "HV0005,B02510,2021-01-01 00:50:00,2021-01-01 01:04:57,42,151,\r\n",
      "\r\n",
      "HV0003,B02764,2021-01-01 00:14:30,2021-01-01 00:50:27,71,226,\r\n",
      "\r\n",
      "HV0003,B02875,2021-01-01 00:22:54,2021-01-01 00:30:20,112,255,\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fec8e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2aa434ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv data into pandas df\n",
    "df_pandas = pd.read_csv('head.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bdb50af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvfhs_license_num        object\n",
       "dispatching_base_num     object\n",
       "pickup_datetime          object\n",
       "dropoff_datetime         object\n",
       "PULocationID              int64\n",
       "DOLocationID              int64\n",
       "SR_Flag                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5cc3816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(hvfhs_license_num,StringType,true),StructField(dispatching_base_num,StringType,true),StructField(pickup_datetime,StringType,true),StructField(dropoff_datetime,StringType,true),StructField(PULocationID,LongType,true),StructField(DOLocationID,LongType,true),StructField(SR_Flag,DoubleType,true)))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convering pandas DF to PySparkDf and extracting the schema\n",
    "# This schema is copied and edited in VS Code\n",
    "spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac33ac89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the dataype of the converted df\n",
    "type(spark.createDataFrame(df_pandas)) #.show() or .schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a11d3e",
   "metadata": {},
   "source": [
    "### To assign the data types we have to use Pyspark datatypes which have to be impoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f87aac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23624871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the edited schema got above to a variable\n",
    "# True is to indicate they can have null values\n",
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f600d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-reading the csv as a PySpark DF with the new Schema\n",
    "pyspark_df = spark.read\\\n",
    "          .option(\"header\", \"true\")\\\n",
    "          .schema(schema)\\\n",
    "          .csv(f'{data_dir}/{data_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "10063261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 33, 44), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 49, 7), PULocationID=230, DOLocationID=166, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 55, 19), dropoff_datetime=datetime.datetime(2021, 1, 1, 1, 18, 21), PULocationID=152, DOLocationID=167, SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 1, 1, 0, 23, 56), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 38, 5), PULocationID=233, DOLocationID=142, SR_Flag=None)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark_df .head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c1ecf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the partition size which will be used awhen writing to the file\n",
    "pyspark_df  = pyspark_df .repartition(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "978e1a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# folder to write the partition into\n",
    "data_partition_dir = 'pq/fhvhv/2021/01'\n",
    "\n",
    "# to STOP overwriting use this\n",
    "# pyspark_df.write.parquet(f'{data_dir}/fhvhv/2021/01/')\n",
    "\n",
    "# to ALLOW overwriting use this\n",
    "# writing the PySpark DF as a parquet file after changing the schema\n",
    "pyspark_df.write.parquet(f'{data_dir}/{data_partition_dir}/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eab49cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\r\n"
     ]
    }
   ],
   "source": [
    "# Counting how many paritions are written to the folder\n",
    "!ls -lh fhvhv/2021/01/ | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1598269c",
   "metadata": {},
   "source": [
    "# Part-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d677d8c3",
   "metadata": {},
   "source": [
    "## Reading back the paruet partition files form memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03ea1d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing all the partitions\n",
    "data_partition_dir = 'fhvhv/2021/01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c14f4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all the parquet partions into a single PySparkDF\n",
    "df = spark.read.parquet(f\"{data_dir}/{data_partition_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d514aea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hvfhs_license_num: string, dispatching_base_num: string, pickup_datetime: timestamp, dropoff_datetime: timestamp, PULocationID: int, DOLocationID: int, SR_Flag: string]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc87d3fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea340649",
   "metadata": {},
   "source": [
    "## BASIC functions for PySPark DF\n",
    "* .select()\n",
    "* .filter()\n",
    "* .take()\n",
    "* .withColumn(new_column_or_old_column_name, function_to_apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "06d4b51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prints the schema in a nice tree like structure\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbd16d6",
   "metadata": {},
   "source": [
    "### Select Function (Columns Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "711e3fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[pickup_datetime: timestamp, dropoff_datetime: timestamp, PULocationID: int, DOLocationID: int]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only certain columns\n",
    "# .select(), .filter(), joins(), groupby is a Transformations which is lazy and not executed immediately\n",
    "df.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27111cf5",
   "metadata": {},
   "source": [
    "### Select function with Filter function (Row selection based on values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7d710d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+------------+\n",
      "|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "|2021-01-05 22:14:07|2021-01-05 22:32:28|         189|         107|\n",
      "|2021-01-02 17:59:55|2021-01-02 18:10:39|          88|         137|\n",
      "|2021-01-02 23:57:54|2021-01-03 00:15:48|         238|         224|\n",
      "|2021-01-06 15:53:13|2021-01-06 16:07:07|         169|         208|\n",
      "|2021-01-07 07:35:24|2021-01-07 07:55:49|          75|          88|\n",
      "|2021-01-07 08:45:12|2021-01-07 08:51:17|         210|         210|\n",
      "|2021-01-02 15:44:26|2021-01-02 16:10:50|         243|          69|\n",
      "|2021-01-04 16:50:28|2021-01-04 16:57:43|         250|         213|\n",
      "|2021-01-03 10:30:34|2021-01-03 10:44:53|          87|          79|\n",
      "|2021-01-03 22:05:20|2021-01-03 22:27:55|          68|         181|\n",
      "|2021-01-04 08:01:02|2021-01-04 08:33:27|          95|         236|\n",
      "|2021-01-02 13:01:10|2021-01-02 13:08:11|         262|         236|\n",
      "|2021-01-06 17:12:27|2021-01-06 17:46:56|         237|          83|\n",
      "|2021-01-04 09:05:18|2021-01-04 09:27:50|         159|          75|\n",
      "|2021-01-06 16:46:47|2021-01-06 17:50:24|         109|         119|\n",
      "|2021-01-06 08:03:47|2021-01-06 08:17:43|         145|         229|\n",
      "|2021-01-04 06:45:42|2021-01-04 06:55:01|         250|         212|\n",
      "|2021-01-03 13:20:41|2021-01-03 13:31:11|         130|          28|\n",
      "|2021-01-03 17:30:33|2021-01-03 17:45:19|          81|          46|\n",
      "|2021-01-06 20:55:57|2021-01-06 21:02:01|         113|          79|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filer & show data\n",
    "# .show(), .take(), head(), write() are Actions which are eager and executed immediately\n",
    "df.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID')\\\n",
    ".filter(df.hvfhs_license_num == 'HV0003')\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "016fedc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(pickup_datetime=datetime.datetime(2021, 1, 5, 22, 14, 7), dropoff_datetime=datetime.datetime(2021, 1, 5, 22, 32, 28), PULocationID=189, DOLocationID=107),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 2, 17, 59, 55), dropoff_datetime=datetime.datetime(2021, 1, 2, 18, 10, 39), PULocationID=88, DOLocationID=137),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 2, 23, 57, 54), dropoff_datetime=datetime.datetime(2021, 1, 3, 0, 15, 48), PULocationID=238, DOLocationID=224),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 6, 15, 53, 13), dropoff_datetime=datetime.datetime(2021, 1, 6, 16, 7, 7), PULocationID=169, DOLocationID=208),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 7, 7, 35, 24), dropoff_datetime=datetime.datetime(2021, 1, 7, 7, 55, 49), PULocationID=75, DOLocationID=88)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filer & show data\n",
    "# .show(), .take(), head(), write() are Actions which are eager and executed immediately\n",
    "df.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID')\\\n",
    ".filter(df.hvfhs_license_num == 'HV0003')\\\n",
    ".head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47054a9f",
   "metadata": {},
   "source": [
    "### take function (Selection of x records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "edaa1611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(pickup_datetime=datetime.datetime(2021, 1, 5, 22, 14, 7), dropoff_datetime=datetime.datetime(2021, 1, 5, 22, 32, 28), PULocationID=189, DOLocationID=107),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 2, 17, 59, 55), dropoff_datetime=datetime.datetime(2021, 1, 2, 18, 10, 39), PULocationID=88, DOLocationID=137),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 2, 23, 57, 54), dropoff_datetime=datetime.datetime(2021, 1, 3, 0, 15, 48), PULocationID=238, DOLocationID=224),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 6, 15, 53, 13), dropoff_datetime=datetime.datetime(2021, 1, 6, 16, 7, 7), PULocationID=169, DOLocationID=208),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 7, 7, 35, 24), dropoff_datetime=datetime.datetime(2021, 1, 7, 7, 55, 49), PULocationID=75, DOLocationID=88)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting records as list\n",
    "df.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID')\\\n",
    ".filter(df.hvfhs_license_num == 'HV0003')\\\n",
    ".take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fac457",
   "metadata": {},
   "source": [
    "## Advanced Functions\n",
    "New Columns can created by using advanced functions on existing columns.\n",
    "All these functions can be imported from the 'functions' module of PySpark\n",
    "\n",
    "***Examples:***\n",
    "* functions.to_data(column_name)\n",
    "* Also you can assign custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15f76fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "553fa5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+\n",
      "|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-----------+------------+------------+------------+\n",
      "| 2021-01-03|  2021-01-03|         255|          34|\n",
      "| 2021-01-05|  2021-01-05|         189|         107|\n",
      "| 2021-01-02|  2021-01-02|          88|         137|\n",
      "| 2021-01-02|  2021-01-02|         238|         224|\n",
      "| 2021-01-06|  2021-01-06|         169|         208|\n",
      "| 2021-01-07|  2021-01-07|          75|          88|\n",
      "| 2021-01-07|  2021-01-07|         210|         210|\n",
      "| 2021-01-02|  2021-01-02|         243|          69|\n",
      "| 2021-01-04|  2021-01-04|         250|         213|\n",
      "| 2021-01-03|  2021-01-03|          87|          79|\n",
      "| 2021-01-03|  2021-01-03|          68|         181|\n",
      "| 2021-01-04|  2021-01-04|          95|         236|\n",
      "| 2021-01-02|  2021-01-02|         262|         236|\n",
      "| 2021-01-04|  2021-01-04|         225|         233|\n",
      "| 2021-01-06|  2021-01-06|         237|          83|\n",
      "| 2021-01-05|  2021-01-05|         231|          87|\n",
      "| 2021-01-06|  2021-01-06|          22|          26|\n",
      "| 2021-01-04|  2021-01-04|         159|          75|\n",
      "| 2021-01-06|  2021-01-06|         109|         119|\n",
      "| 2021-01-06|  2021-01-06|         145|         229|\n",
      "+-----------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df\\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime))\\\n",
    "    .withColumn('dropoff_date', F.to_date(df.pickup_datetime))\\\n",
    "    .select('pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a70cf90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
